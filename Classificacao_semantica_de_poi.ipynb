{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificacao_semantica_de_poi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxRe0ToKKzjKtoAC3eIXtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiocapanema/minicurso_gnn_sbrc2022/blob/main/Classificacao_semantica_de_poi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spektral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhK1ZsRrm9Dx",
        "outputId": "3cd23b0e-daa7-4b5e-a5ab-0058d99ac70e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spektral\n",
            "  Downloading spektral-1.1.0-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 129 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral) (1.1.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from spektral) (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.44.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.0.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (13.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.24.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (4.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->spektral) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Installing collected packages: tf-estimator-nightly, spektral\n",
            "Successfully installed spektral-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "llfk2z_SaeBo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "from spektral.data import BatchLoader, PackedBatchLoader\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "\n",
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.datasets.citation import Citation\n",
        "from spektral.layers import ARMAConv, GraphMasking\n",
        "from spektral.transforms import LayerPreprocess\n",
        "\n",
        "import json\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "\n",
        "import spektral as sk\n",
        "\n",
        "from os import path as osp\n",
        "from urllib.error import URLError\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "from spektral.data import Dataset, Graph\n",
        "from spektral.datasets.utils import download_file\n",
        "from spektral.utils import io, sparse\n",
        "import random\n",
        "\n",
        "def idx_bfs(adjacency, k):\n",
        "\n",
        "    g = nx.from_numpy_matrix(np.matrix(adjacency), parallel_edges=False)\n",
        "    nodes_centrality = nx.algorithms.centrality.degree_centrality(g)\n",
        "    nodes_centrality = [[i, j] for i, j in zip(nodes_centrality.keys(), nodes_centrality.values())]\n",
        "    nodes_centrality = sorted(nodes_centrality, key=lambda e: e[1], reverse=True)\n",
        "    central_node = nodes_centrality[0][0]\n",
        "    bfs = nx.bfs_tree(g, source=central_node, depth_limit=k)\n",
        "    count = 0\n",
        "    idx = []\n",
        "    for u, v, weight in bfs.edges(data=\"weight\"):\n",
        "\n",
        "        if u not in idx:\n",
        "            idx.append(u)\n",
        "        if len(idx) == k:\n",
        "            break\n",
        "        if v not in idx:\n",
        "            idx.append(v)\n",
        "        if len(idx) == k:\n",
        "            break\n",
        "\n",
        "    idx = np.array(idx)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def top_k_nodes(adjacency, node_features, labels, k):\n",
        "\n",
        "    row_sum = []\n",
        "    total = sum(adjacency.flatten())\n",
        "\n",
        "    binary_matrix = []\n",
        "    for i in range(len(adjacency)):\n",
        "        binary = []\n",
        "        for j in adjacency[i]:\n",
        "            if j != 0:\n",
        "                binary.append(1)\n",
        "            else:\n",
        "                binary.append(0)\n",
        "        binary_matrix.append(binary)\n",
        "    binary_matrix = np.array(binary_matrix)\n",
        "    total_binary = sum(binary_matrix.flatten())\n",
        "\n",
        "    for i in range(len(adjacency)):\n",
        "        b_score = np.sum(binary_matrix[i])/total_binary\n",
        "        w_score = np.sum(adjacency[i])/total\n",
        "        row_sum.append([ (b_score * w_score)/(b_score + w_score) , i])\n",
        "\n",
        "    # g = nx.from_numpy_matrix(np.matrix(adjacency), parallel_edges=False)\n",
        "    # nodes_centrality = nx.algorithms.centrality.degree_centrality(g)\n",
        "    # nodes_centrality = [[i, j] for i, j in zip(nodes_centrality.keys(), nodes_centrality.values())]\n",
        "    # nodes_centrality = sorted(nodes_centrality, key=lambda e: e[1], reverse=True)\n",
        "    # central_node = nodes_centrality[0][0]\n",
        "    # bfs = nx.bfs_tree(g, source=central_node, depth_limit=k)\n",
        "    # count = 0\n",
        "    # idx = []\n",
        "    # for u, v, weight in bfs.edges(data=\"weight\"):\n",
        "    #\n",
        "    #     if u not in idx:\n",
        "    #         idx.append(u)\n",
        "    #     if len(idx) == k:\n",
        "    #         break\n",
        "    #     if v not in idx:\n",
        "    #         idx.append(v)\n",
        "    #     if len(idx) == k:\n",
        "    #         break\n",
        "    #\n",
        "    # idx = np.array(idx)\n",
        "\n",
        "    row_sum = sorted(row_sum, reverse=True, key=lambda e:e[0])\n",
        "    # if len(row_sum) > k:\n",
        "    # if row_sum[k][0] < 4:\n",
        "    #     print(\"ola\")\n",
        "    row_sum = row_sum[:k+20]\n",
        "\n",
        "    idx = np.array([e[1] for e in row_sum])\n",
        "    idx = np.array(idx)\n",
        "    adjacency = adjacency[idx[:, None], idx]\n",
        "    node_features = node_features[idx]\n",
        "    labels = labels[idx]\n",
        "    idx = idx_bfs(adjacency, k)\n",
        "    random.seed(1)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "\n",
        "    adjacency = adjacency[idx[:, None], idx]\n",
        "    node_features = node_features[idx]\n",
        "    labels = labels[idx]\n",
        "    for l in adjacency:\n",
        "        if sum(l) == 0:\n",
        "            print(\"ee\")\n",
        "            exit()\n",
        "\n",
        "    return adjacency, node_features, labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PoICategoryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    The Benchmark Data Sets for Graph Kernels from TU Dortmund\n",
        "    ([link](https://chrsmrrs.github.io/datasets/docs/datasets/)).\n",
        "\n",
        "    Node features are computed by concatenating the following features for\n",
        "    each node:\n",
        "\n",
        "    - node attributes, if available;\n",
        "    - node labels, if available, one-hot encoded.\n",
        "\n",
        "    Some datasets might not have node features at all. In this case, attempting\n",
        "    to use the dataset with a Loader will result in a crash. You can create\n",
        "    node features using some of the transforms available in `spektral.transforms`\n",
        "    or you can define your own features by accessing the individual samples in\n",
        "    the `graph` attribute of the dataset (which is a list of `Graph` objects).\n",
        "\n",
        "    Edge features are computed by concatenating the following features for\n",
        "    each node:\n",
        "\n",
        "    - edge attributes, if available;\n",
        "    - edge labels, if available, one-hot encoded.\n",
        "\n",
        "    Graph labels are provided for each dataset.\n",
        "\n",
        "    Specific details about each individual dataset can be found in\n",
        "    `~/.spektral/datasets/TUDataset/<dataset name>/README.md`, after the dataset\n",
        "    has been downloaded locally (datasets are downloaded automatically upon\n",
        "    calling `TUDataset('<dataset name>')` the first time).\n",
        "\n",
        "    **Arguments**\n",
        "\n",
        "    - `name`: str, name of the dataset to load (see `TUD.available_datasets`).\n",
        "    - `clean`: if `True`, rload a version of the dataset with no isomorphic\n",
        "               graphs.\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/tree/main/datasets\"\n",
        "    url_clean = (\n",
        "        \"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/tree/main/datasets\"\n",
        "    )\n",
        "\n",
        "    def __init__(self, name, n_samples, max_nodes, clean=False, **kwargs):\n",
        "\n",
        "        self.name = name\n",
        "        self.max_nodes = max_nodes\n",
        "        self.n_samples = n_samples\n",
        "        self.clean = clean\n",
        "        print(\"caminho\", self.path)\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def path(self):\n",
        "        return osp.join(super().path, self.name + (\"_clean\" if self.clean else \"\"))\n",
        "\n",
        "    def download(self):\n",
        "        print(\n",
        "            \"Downloading {} dataset{}.\".format(\n",
        "                self.name, \" (clean)\" if self.clean else \"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        url = \"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/tree/main/datasets/{}\"\n",
        "        print(url.format(\"PoiCategory_dataset.zip\"))\n",
        "        import requests\n",
        "        # req = requests.get(\"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/blob/main/datasets/PoiCategory_dataset.zip\")\n",
        "        # req = requests.get(\"https://drive.google.com/file/d/1W5YqIPtif7l1ytQmYLZS7N1HNYz57uM5/view?usp=sharing\", stream=True)\n",
        "        #\n",
        "        # print(req.content)\n",
        "        # with tarfile.open(fileobj=BytesIO(req.content), mode=\"r:gz\") as tar_file:\n",
        "        #     for member in tar_file.getmembers():\n",
        "        #         f = tar_file.extractfile(member)\n",
        "        # df = pd.read_csv(f)\n",
        "        # print(df)\n",
        "        # #file = wget.download(\"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/blob/main/datasets/PoiCategory_dataset.zip\")\n",
        "        # #file = ZipFile(\"/home/claudio/Documentos/pycharm_projects/minicurso/PoiCategory_dataset.zip\")\n",
        "        # #print(file.namelist())\n",
        "        #\n",
        "        # adjacency = file.open(\"PoiCategory_dataset/adjacency.csv\")\n",
        "        # node_features = file.open(\"PoiCategory_dataset/node_features.csv\")\n",
        "        # adjacency = pd.read_csv(adjacency)\n",
        "        # node_features = pd.read_csv(node_features)\n",
        "\n",
        "        # download file and write it to self.path\n",
        "        Path(self.path).mkdir(parents=True, exist_ok=True)\n",
        "        adjacency = pd.read_csv(\"/home/claudio/Documentos/pycharm_projects/minicurso_gnn_sbrc2022/datasets/adjacency.zip\", compression=\"zip\")\n",
        "        adjacency.to_csv(self.path + \"/adjacency.csv\", index=False)\n",
        "        node_features = pd.read_csv(\n",
        "            \"/home/claudio/Documentos/pycharm_projects/minicurso_gnn_sbrc2022/datasets/node_features.zip\", compression=\"zip\")\n",
        "        node_features.to_csv(self.path + \"/node_features.csv\", index=False)\n",
        "\n",
        "        # adjacency = pd.read_csv(\"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/blob/main/datasets/adjacency.zip\", compression=\"zip\")\n",
        "        # adjacency.to_csv(self.path + \"/adjacency.csv\", index=False)\n",
        "        # node_features = pd.read_csv(\n",
        "        #     \"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/blob/main/datasets/node_features.zip\", compression=\"zip\")\n",
        "        # node_features.to_csv(self.path + \"/node_features.csv\", index=False)\n",
        "\n",
        "    def read(self):\n",
        "\n",
        "\n",
        "        # Convert to Graph\n",
        "        a_list, x_list, labels = self.poi_dataset(self.n_samples, self.max_nodes)\n",
        "        e_list = [None] * len(a_list)\n",
        "        print(\"Successfully loaded {}.\".format(self.name))\n",
        "        return [\n",
        "            Graph(x=x, a=a, e=e, y=y)\n",
        "            for x, a, e, y in zip(x_list, a_list, e_list, labels)\n",
        "        ]\n",
        "\n",
        "    def poi_dataset(self, max_samples, max_nodes):\n",
        "\n",
        "        A_df = pd.read_csv(self.path + \"/adjacency.csv\").dropna(how='any', axis=0)\n",
        "        X_df = pd.read_csv(self.path + \"/node_features.csv\").dropna(how='any', axis=0)\n",
        "        print(\"quantidade original\", len(A_df))\n",
        "        userid = A_df['user_id'].tolist()[:max_samples]\n",
        "        matrix_df = A_df['matrices'].tolist()\n",
        "        temporal_df = X_df['matrices'].tolist()\n",
        "        category_df = A_df['category'].tolist()\n",
        "\n",
        "        A_list = []\n",
        "        X_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for i in range(len(userid)):\n",
        "            adjacency = matrix_df[i]\n",
        "            labels = category_df[i]\n",
        "            adjacency = json.loads(adjacency)\n",
        "            if len(adjacency) < 2:\n",
        "                continue\n",
        "\n",
        "            labels = json.loads(labels)\n",
        "            labels = np.array(labels)\n",
        "            node_features = temporal_df[i]\n",
        "            node_features = json.loads(node_features)\n",
        "            node_features = np.array(node_features).astype(np.float)\n",
        "            adjacency = np.array(adjacency).astype(np.float)\n",
        "            adjacency, node_features, labels = top_k_nodes(adjacency, node_features, labels, max_nodes)\n",
        "            if len(adjacency) < max_nodes:\n",
        "                continue\n",
        "            adjacency = sk.layers.ARMAConv.preprocess(adjacency)\n",
        "\n",
        "            A_list.append(adjacency)\n",
        "            X_list.append(node_features)\n",
        "            labels_list.append(np.array(np_utils.to_categorical(labels, num_classes=7)))\n",
        "\n",
        "        print(\"aa\", len(A_list))\n",
        "\n",
        "        return A_list, X_list, labels_list\n",
        "\n",
        "    @staticmethod\n",
        "    def available_datasets():\n",
        "        url = \"https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/tree/main/datasets/\"\n",
        "        url = \"https://chrsmrrs.github.io/datasets/docs/datasets/{}\"\n",
        "        try:\n",
        "            tables = pd.read_parquet(url.format(\"adjacency.parquet\"))\n",
        "            names = []\n",
        "            for table in tables:\n",
        "                names.extend(table.Name[1:].values.tolist())\n",
        "            return names\n",
        "        except URLError:\n",
        "            # No internet, don't panic\n",
        "            print(\"Could not read URL {}\".format(url))\n",
        "            return []\n",
        "\n",
        "\n",
        "def _normalize(x, norm=None):\n",
        "    \"\"\"\n",
        "    Apply one-hot encoding or z-score to a list of node features\n",
        "    \"\"\"\n",
        "    if norm == \"ohe\":\n",
        "        fnorm = OneHotEncoder(sparse=False, categories=\"auto\")\n",
        "    elif norm == \"zscore\":\n",
        "        fnorm = StandardScaler()\n",
        "    else:\n",
        "        return x\n",
        "    return fnorm.fit_transform(x)\n",
        "\n",
        "\n",
        "############################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def one_hot_decoding_predicted2(data):\n",
        "\n",
        "    new = []\n",
        "    for g in data:\n",
        "        node_label = []\n",
        "        for node in g:\n",
        "            node_label.append(np.argmax(node))\n",
        "        new.append(node_label)\n",
        "\n",
        "    new = np.array(new).flatten()\n",
        "    return new\n",
        "\n",
        "channels = 16  # Number of channels in the first layer\n",
        "iterations = 1  # Number of iterations to approximate each ARMA(1)\n",
        "order = 2  # Order of the ARMA filter (number of parallel stacks)\n",
        "share_weights = True  # Share weights in each ARMA stack\n",
        "dropout_skip = 0.75  # Dropout rate for the internal skip connection of ARMA\n",
        "dropout = 0.5  # Dropout rate for the features\n",
        "l2_reg = 5e-5  # L2 regularization rate\n",
        "learning_rate = 1e-2  # Learning rate\n",
        "epochs = 20  # Number of training epochs\n",
        "patience = 100  # Patience for early stopping\n",
        "\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mask = GraphMasking()\n",
        "        self.conv1 = ARMAConv(\n",
        "        channels,\n",
        "        iterations=iterations,\n",
        "        order=order,\n",
        "        share_weights=share_weights,\n",
        "        dropout_rate=dropout_skip,\n",
        "        activation=\"elu\",\n",
        "        gcn_activation=\"elu\",\n",
        "        kernel_regularizer=l2(l2_reg)\n",
        "    )\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.conv2 = ARMAConv(\n",
        "        n_out,\n",
        "        iterations=1,\n",
        "        order=1,\n",
        "        share_weights=share_weights,\n",
        "        dropout_rate=dropout_skip,\n",
        "        activation=\"softmax\",\n",
        "        gcn_activation=None,\n",
        "        kernel_regularizer=l2(l2_reg),\n",
        "    )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a = inputs\n",
        "        x = self.mask(x)\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.dropout(x)\n",
        "        output = self.conv2([x, a])\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Press the green button in the gutter to run the script.\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    dataset = PoICategoryDataset(\"PoICategoryDataset\", n_samples=20000, max_nodes=3, clean=True)\n",
        "    batch_size = 50  # Batch size\n",
        "    # Parameters\n",
        "    N = max(g.n_nodes for g in dataset)\n",
        "    F = dataset.n_node_features  # Dimension of node features\n",
        "    S = dataset.n_edge_features  # Dimension of edge features\n",
        "    n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "    print(\"Parametros\")\n",
        "    print(N, F, S, n_out)\n",
        "\n",
        "    np.random.seed(seed=1)\n",
        "    idxs = np.random.permutation(len(dataset))\n",
        "    print(\"ta 1\", len(dataset))\n",
        "    train = int(len(idxs) * 0.8)\n",
        "    dataset_train = dataset[:train]\n",
        "    dataset_test = dataset[train:]\n",
        "\n",
        "    print(\"ta \", len(dataset_test))\n",
        "\n",
        "    a_train_list = []\n",
        "    x_train_list = []\n",
        "    y_train = []\n",
        "\n",
        "    for g in dataset_train:\n",
        "        a_train_list.append(g.a)\n",
        "        x_train_list.append(g.x)\n",
        "        y_train.append(g.y)\n",
        "\n",
        "    a_test_list = []\n",
        "    x_test_list = []\n",
        "    y_test = []\n",
        "\n",
        "    for g in dataset_test:\n",
        "        a_test_list.append(g.a)\n",
        "        x_test_list.append(g.x)\n",
        "        y_test.append(g.y)\n",
        "\n",
        "    print(\"quantidade grafos de teste: \", len(dataset_train))\n",
        "    print(\"tamanho: \", len(y_test))\n",
        "\n",
        "    a_train_list = np.array(a_train_list)\n",
        "    a_test_list = np.array(a_test_list)\n",
        "    x_train_list = np.array(x_train_list)\n",
        "    x_test_list = np.array(x_test_list)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    inputs_train = [x_train_list, a_train_list]\n",
        "    inputs_test = [x_test_list, a_test_list]\n",
        "\n",
        "\n",
        "\n",
        "    channels = 16  # Number of channels in the first layer\n",
        "    iterations = 1  # Number of iterations to approximate each ARMA(1)\n",
        "    order = 2  # Order of the ARMA filter (number of parallel stacks)\n",
        "    share_weights = True  # Share weights in each ARMA stack\n",
        "    dropout_skip = 0.75  # Dropout rate for the internal skip connection of ARMA\n",
        "    dropout = 0.3  # Dropout rate for the features\n",
        "    l2_reg = 5e-5  # L2 regularization rate\n",
        "    learning_rate = 0.0001  # Learning rate\n",
        "    epochs = 30  # Number of training epochs\n",
        "    patience = 10  # Patience for early stopping\n",
        "    a_dtype = dataset[0].a.dtype  # Only needed for TF 2.1\n",
        "\n",
        "\n",
        "    # Model definition\n",
        "\n",
        "    x_in = Input(shape=(N, F))\n",
        "    a_in = Input((N, N), sparse=False, dtype=a_dtype)\n",
        "\n",
        "    gc_1 = ARMAConv(\n",
        "        7,\n",
        "        iterations=iterations,\n",
        "        order=order,\n",
        "        share_weights=share_weights,\n",
        "        dropout_rate=dropout_skip,\n",
        "        activation=\"softmax\",\n",
        "        gcn_activation=\"gelu\",\n",
        "        kernel_regularizer=l2(l2_reg),\n",
        "    )([x_in, a_in])\n",
        "    gc_2 = Dropout(dropout)(gc_1)\n",
        "    gc_2 = ARMAConv(\n",
        "        n_out,\n",
        "        iterations=1,\n",
        "        order=1,\n",
        "        share_weights=share_weights,\n",
        "        dropout_rate=dropout_skip,\n",
        "        activation=\"softmax\",\n",
        "        gcn_activation=None,\n",
        "        kernel_regularizer=l2(l2_reg),\n",
        "    )([gc_2, a_in])\n",
        "\n",
        "    dense_out = Dense(n_out, activation=\"softmax\")(x_in)\n",
        "\n",
        "    out = tf.Variable(1.) * gc_2 + tf.Variable(1.) * dense_out\n",
        "\n",
        "    # Build model\n",
        "    model = Model(inputs=[x_in, a_in], outputs=[out])\n",
        "    model.summary()\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", weighted_metrics=[\"acc\"]\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"treinar\")\n",
        "    model.fit(\n",
        "        x=inputs_train,\n",
        "        y=y_train,\n",
        "        validation_data=(inputs_test, y_test),\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "    )\n",
        "\n",
        "    ################################################################################\n",
        "    # Evaluate model\n",
        "    ################################################################################\n",
        "    print(\"Testing model\")\n",
        "    loss, acc = model.evaluate(x=inputs_test, y=y_test)\n",
        "    print(\"Done. Test loss: {}. Test acc: {}\".format(loss, acc))\n",
        "    predictions = model.predict(x=inputs_test)\n",
        "\n",
        "    predictions = one_hot_decoding_predicted2(predictions)\n",
        "    y_test = one_hot_decoding_predicted2(y_test)\n",
        "    print(\"Unicos: \", pd.Series(predictions).unique().tolist())\n",
        "    report = skm.classification_report(y_test, predictions, target_names=['Shopping', 'Community', 'Food', 'Entertainment', 'Travel', 'Outdoors',\n",
        "                                     'Nightlife'], output_dict=True)\n",
        "    print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "L2GOjfMGktE4",
        "outputId": "875ea19d-248f-4a49-e471-70ad416746b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caminho /root/spektral/datasets/PoICategoryDataset/PoICategoryDataset_clean\n",
            "Downloading PoICategoryDataset dataset (clean).\n",
            "https://github.com/claudiocapanema/minicurso_gnn_sbrc2022/tree/main/datasets/PoiCategory_dataset.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-357f16faceaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoICategoryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoICategoryDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m  \u001b[0;31m# Batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-92e368e2a8a1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, n_samples, max_nodes, clean, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"caminho\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spektral/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transforms, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Download data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Read graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-92e368e2a8a1>\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# download file and write it to self.path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0madjacency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/claudio/Documentos/pycharm_projects/minicurso_gnn_sbrc2022/datasets/adjacency.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0madjacency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/adjacency.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         node_features = pd.read_csv(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;31m# ZIP Compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_BytesZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;31m# TextIOBase, TextIOWrapper, mmap]]]\"; expected \"Union[Union[str,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# _PathLike[str]], IO[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_zip\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/claudio/Documentos/pycharm_projects/minicurso_gnn_sbrc2022/datasets/adjacency.zip'"
          ]
        }
      ]
    }
  ]
}