{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificacao_semantica_de_poi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKTta/x7RZ46JV2lRaW9Zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiocapanema/minicurso_gnn_sbrc2022/blob/main/Classificacao_semantica_de_poi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral\n",
        "!pip install pandas\n",
        "\n",
        "!wget https://raw.githubusercontent.com/claudiocapanema/minicurso_gnn_sbrc2022/main/datasets/adjacency.zip\n",
        "!wget https://raw.githubusercontent.com/claudiocapanema/minicurso_gnn_sbrc2022/main/datasets/node_features.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhK1ZsRrm9Dx",
        "outputId": "ed5f081b-c22d-4e54-e2db-615039b2f108"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spektral in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral) (1.1.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from spektral) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.21.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral) (1.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.17.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.46.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (14.0.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (4.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.25.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->spektral) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->spektral) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "--2022-05-20 04:00:27--  https://raw.githubusercontent.com/claudiocapanema/minicurso_gnn_sbrc2022/main/datasets/adjacency.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14624769 (14M) [application/zip]\n",
            "Saving to: ‘adjacency.zip.1’\n",
            "\n",
            "adjacency.zip.1     100%[===================>]  13.95M  88.6MB/s    in 0.2s    \n",
            "\n",
            "2022-05-20 04:00:28 (88.6 MB/s) - ‘adjacency.zip.1’ saved [14624769/14624769]\n",
            "\n",
            "--2022-05-20 04:00:28--  https://raw.githubusercontent.com/claudiocapanema/minicurso_gnn_sbrc2022/main/datasets/node_features.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5709977 (5.4M) [application/zip]\n",
            "Saving to: ‘node_features.zip’\n",
            "\n",
            "node_features.zip   100%[===================>]   5.45M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-20 04:00:28 (47.2 MB/s) - ‘node_features.zip’ saved [5709977/5709977]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tensorflow.keras import utils as np_utils\n",
        "import spektral as sk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "from spektral.data import Dataset, Graph\n",
        "\n",
        "from spektral.data import BatchLoader, PackedBatchLoader\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "QYOuiRMIQuO6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoiCategoryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    The PoICategory Dataset\n",
        "    **Arguments**\n",
        "    - `name`: str, name of the dataset to load.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name, **kwargs):\n",
        "        self.name = name\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def poi_dataset(self, max_samples=5000):\n",
        "\n",
        "        A_df = pd.read_csv(\"https://raw.githubusercontent.com/claudiocapanema/minicurso_gnn_sbrc2022/main/datasets/adjacency.zip\", compression='zip').dropna(how='any', axis=0)\n",
        "        X_df = pd.read_csv(\"https://raw.githubusercontent.com/claudiocapanema/minicurso_gnn_sbrc2022/main/datasets/node_features.zip\", compression='zip').dropna(how='any', axis=0)\n",
        "        print(\"Original number of graphs\", len(A_df))\n",
        "        userid = A_df['user_id'].tolist()[:max_samples]\n",
        "        matrix_df = A_df['matrices'].tolist()\n",
        "        temporal_df = X_df['matrices'].tolist()\n",
        "        category_df = A_df['category'].tolist()\n",
        "\n",
        "        A_list = []\n",
        "        X_list = []\n",
        "        labels_labels_list = []\n",
        "        count_nodes = 0\n",
        "\n",
        "        for i in range(len(userid)):\n",
        "            adjacency = matrix_df[i]\n",
        "            labels = category_df[i]\n",
        "            adjacency = json.loads(adjacency)\n",
        "            if len(adjacency) < 2:\n",
        "                continue\n",
        "\n",
        "            labels = json.loads(labels)\n",
        "            labels = np.array(labels)\n",
        "            node_features = temporal_df[i]\n",
        "            node_features = json.loads(node_features)\n",
        "            node_features = np.array(node_features).astype(float)\n",
        "            node_features = _normalize(node_features)\n",
        "            adjacency = np.array(adjacency).astype(float)\n",
        "\n",
        "\n",
        "            labels = np.array(np_utils.to_categorical(labels, num_classes=7))\n",
        "            labels_labels_list.append(labels)\n",
        "\n",
        "            indice = np.argmax(np.sum(adjacency, axis=1))\n",
        "            \"\"\" Change the pre-processing based on the used message passing layer \"\"\"\n",
        "            adjacency = sk.layers.ARMAConv.preprocess(adjacency)\n",
        "            count_nodes += len(adjacency)\n",
        "            A_list.append(adjacency)\n",
        "            X_list.append(node_features)\n",
        "\n",
        "        print(\"Total of nodes: \", count_nodes)\n",
        "\n",
        "        A_list, X_list, labels_list = np.array(A_list), np.array(X_list), np.array(labels_labels_list)\n",
        "\n",
        "        print(\"A: \", A_list.shape, \" X: \", X_list.shape, \" Labels: \", labels_list.shape)\n",
        "\n",
        "        return A_list, X_list, labels_list\n",
        "\n",
        "    def read(self):\n",
        "\n",
        "        # Convert to Graph\n",
        "        a_list, x_list, labels = self.poi_dataset()\n",
        "        print(\"Successfully loaded {}.\".format(self.name))\n",
        "        e_list = [None] * len(a_list)\n",
        "        return [\n",
        "            Graph(x=x, a=a, e=e, y=y)\n",
        "            for x, a, e, y in zip(x_list, a_list, e_list, labels)\n",
        "        ]\n",
        "\n",
        "\n",
        "def _normalize(x, norm=None):\n",
        "    \"\"\"\n",
        "    Apply one-hot encoding or z-score to a list of node features\n",
        "    \"\"\"\n",
        "    if norm == \"ohe\":\n",
        "        fnorm = OneHotEncoder(sparse=False, categories=\"auto\")\n",
        "    elif norm == \"zscore\":\n",
        "        fnorm = StandardScaler()\n",
        "    else:\n",
        "        return x\n",
        "    return fnorm.fit_transform(x)"
      ],
      "metadata": {
        "id": "pyDK3NKRQ3Bm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "RHwqQQx4W1KE",
        "outputId": "43f0348c-ad54-4529-dcfd-652698d1d67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adjacency.parquet  adjacency.zip  adjacency.zip.1  node_features.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.layers import ARMAConv, GraphMasking, GCNConv\n",
        "\n",
        "class GNN(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mask = GraphMasking()\n",
        "        self.conv1 = ARMAConv(\n",
        "        16,\n",
        "        iterations=1,\n",
        "        order=2,\n",
        "        share_weights=True,\n",
        "        dropout_rate=0.75,\n",
        "        activation=\"elu\",\n",
        "        gcn_activation=\"elu\",\n",
        "        kernel_regularizer=l2(5e-5)\n",
        "    )\n",
        "        self.dropout = Dropout(0.6)\n",
        "        self.conv2 = ARMAConv(\n",
        "        7,\n",
        "        iterations=1,\n",
        "        order=1,\n",
        "        share_weights=True,\n",
        "        dropout_rate=0.75,\n",
        "        activation=\"softmax\",\n",
        "        gcn_activation=None,\n",
        "        kernel_regularizer=l2(5e-5),\n",
        "    )\n",
        "    def call(self, inputs):\n",
        "        X_input, A_input = inputs\n",
        "        X = self.mask(X_input)\n",
        "        X = self.conv1([X, A_input])\n",
        "        X = self.dropout(X)\n",
        "        output = self.conv2([X, A_input])\n",
        "        return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    dataset = PoiCategoryDataset(\"PoICategoryDataset\", n_samples=10000)\n",
        "\n",
        "    # Parameters\n",
        "    N = max(g.n_nodes for g in dataset)\n",
        "    D = dataset.n_node_features  # Dimension of node features\n",
        "    S = dataset.n_edge_features  # Dimension of edge features\n",
        "    n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "    print(\"Parameters\")\n",
        "    print(N, D, S, n_out)\n",
        "\n",
        "    np.random.seed(seed=1)\n",
        "    # shuffle data\n",
        "    idxs = np.random.permutation(len(dataset))\n",
        "    split_va, split_te = int(0.8 * len(dataset)), int(0.9 * len(dataset))\n",
        "    idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
        "    dataset_train = dataset[idx_tr]\n",
        "    dataset_validation = dataset[idx_va]\n",
        "    dataset_test = dataset[idx_te]\n",
        "    batch_size = 5  # Batch size\n",
        "    epochs = 10\n",
        "    # The data have already been shuffled\n",
        "    loader_tr = BatchLoader(dataset_train, epochs=10, batch_size=batch_size, mask=True, node_level=True,\n",
        "                            shuffle=False)\n",
        "    loader_va = BatchLoader(dataset_validation, epochs=10, batch_size=batch_size, mask=True, node_level=True,\n",
        "                            shuffle=False)\n",
        "    loader_te = BatchLoader(dataset_test, epochs=10, batch_size=batch_size, mask=True, node_level=True,\n",
        "                            shuffle=False)\n",
        "\n",
        "    model = GNN()\n",
        "    opt = Adam(lr=0.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"acc\"])\n",
        "\n",
        "    model.fit(\n",
        "        loader_tr.load(),\n",
        "        steps_per_epoch=loader_tr.steps_per_epoch,\n",
        "        epochs=epochs,\n",
        "        validation_data=loader_va.load(),\n",
        "        validation_steps=loader_va.steps_per_epoch,\n",
        "        callbacks=[EarlyStopping(patience=3,\n",
        "                                 restore_best_weights=True)],\n",
        "    )\n",
        "\n",
        "    ################################################################################\n",
        "    # Evaluate model\n",
        "    ################################################################################\n",
        "    print(\"Testing model\")\n",
        "    loss, acc = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
        "    print(\"Done. Test loss: {}. Test acc: {}\".format(loss, acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3SthEIVRaWi",
        "outputId": "43d7a8ee-1131-4aeb-ca7f-1fa77de32957"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of graphs 16448\n",
            "Total of nodes:  461888\n",
            "A:  (5000,)  X:  (5000,)  Labels:  (5000,)\n",
            "Successfully loaded PoICategoryDataset.\n",
            "Parameters\n",
            "395 48 None 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800/800 [==============================] - 15s 16ms/step - loss: 1.1025 - acc: 0.1850 - val_loss: 0.8902 - val_acc: 0.3368\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 11s 14ms/step - loss: 0.9996 - acc: 0.2538 - val_loss: 0.8536 - val_acc: 0.3811\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 12s 15ms/step - loss: 0.9378 - acc: 0.3066 - val_loss: 0.8352 - val_acc: 0.3907\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 11s 14ms/step - loss: 0.8988 - acc: 0.3399 - val_loss: 0.8262 - val_acc: 0.3936\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 12s 15ms/step - loss: 0.8727 - acc: 0.3613 - val_loss: 0.8201 - val_acc: 0.3951\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 12s 15ms/step - loss: 0.8546 - acc: 0.3765 - val_loss: 0.8160 - val_acc: 0.3965\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 12s 15ms/step - loss: 0.8422 - acc: 0.3866 - val_loss: 0.8128 - val_acc: 0.3967\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 12s 15ms/step - loss: 0.8321 - acc: 0.3941 - val_loss: 0.8099 - val_acc: 0.3967\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 11s 14ms/step - loss: 0.8249 - acc: 0.3980 - val_loss: 0.8074 - val_acc: 0.3968\n",
            "Epoch 10/10\n",
            "798/800 [============================>.] - ETA: 0s - loss: 0.8192 - acc: 0.4010WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "800/800 [==============================] - 12s 14ms/step - loss: 0.8190 - acc: 0.4011 - val_loss: 0.7993 - val_acc: 0.3993\n",
            "Testing model\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 0.7878 - acc: 0.4035\n",
            "Done. Test loss: 0.7877535820007324. Test acc: 0.40349307656288147\n"
          ]
        }
      ]
    }
  ]
}